<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Primer on Binary Classification in Supervised Learning</title>
    <link rel="stylesheet" href="../css/styles.css"> <!-- Ensure the path is correct -->
    <script src="/js/script.js"></script> <!-- Ensure the path is correct -->
</head>
<body>
    <div id="mySidenav" class="nav-menu">
        <button class="closebtn" onclick="closeNav()">&times;</button>
        <a href="/index.html">Home</a>
        <a href="/articles.html">Articles</a>
        <a href="/blog.html">Blog</a>
        <a href="/contact.html">Contact Me</a>
    </div>

    <aside>
        <button class="hamburger" onclick="openNav()" aria-label="Open navigation menu" onKeyDown="handleKeyPress(event)">
            &#9776;
        </button>
    </aside>

    <main>
        <article>
            <h2>A Primer on Binary Classification in Supervised Learning</h2>
            <h3>Annotated by: <strong>Noah Schliesman</strong></h3>
            <p>
                When studying transformers, it is easiest to start with the formulation of supervised learning.
                Let’s explore our first equation, the objective function, by explaining each variable first,
                examining what we want our equation to do, and then give the equation.
            </p>
            <p>
                <strong>Training Dataset:</strong> We have a training dataset, and a sample with its corresponding label.
                We can describe the set of these labeled pairs as \( \{ (x_i, y_i) \}_{i=1}^n = X \).
            </p>
            <p>
                Here’s a quick MATLAB plot of a simulated training dataset, where \( x_i = [Feature_1, Feature_2] \) and \( y_i \in \{0, 1\} \).
            </p>
            <p>
                Deep learning incites us to find a set of parameters, \( \Theta \). The term parameter is daunting,
                but it’s really just a set of knobs or controls that adjust our predictions. These can also
                be known as our weights and biases.
            </p>
            <p>
                To optimize our set of parameters, \( \Theta \), we must first discuss the usage of a predefined
                loss function. We have countless candidates and the choice of a loss function should be
                highly dependent on the application at hand.
            </p>
            <p>
                The simplest and perhaps most intuitive loss function is maximum likelihood estimation.
                We want to find a likelihood function that measures the probability of classifying a label
                \( y_i \), given the training sample \( x_i \) and set of parameters \( \Theta \).
            </p>
            <p>
                Due to properties of logarithms and the usage of a more stable sum, we take the log:
                \( \ell(\Theta) = \sum_{i=1}^N \log(p(y_i|x_i, \Theta)) \)
            </p>
            <p>
                This is not ideal. Don’t worry, we have a bunch of tools to increase likelihood and create
                a more robust model. One of them is regularization, of which we prevent the model from
                overfitting by penalizing the loss function based on the magnitude of model parameters.
            </p>
            <p>
                That's a little better, so now let’s add an optimization method. Let’s start with gradient
                descent.
            </p>
            <p>
                At this point, likelihood isn’t necessarily the best way to determine the strength of a
                model. We’re going to create a training and test split for our dataset. This will tell us the
                correctness of our binary classification for a real task. We choose a 60:40 split.
            </p>
            <p>
                Don’t get too excited. Our input data points are somewhat arbitrary and this could be a
                sign of overfitting. Without good data, a neural network is worthless. Let’s add
                exponentially more data around two centers for binary classification.
            </p>
            <p>
                That’s really good. By using a simple averaged Bayesian log-likelihood as our loss
                function, L2 ridge regulation to penalize our loss function, and gradient descent to
                optimize our weights and biases, our network has learned to classify a synthetic
                dataset.
            </p>
            <h4>Appendix:</h4>
            <p>
                Refer to the attached MATLAB scripts for implementations of the described models and methods.
            </p>
        </article>
    </main>

    <footer>
        <p>&copy; 2024 Pay Attention. All rights reserved.</p>
    </footer>
</body>
</html>
