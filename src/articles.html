<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pay Attention</title>
    <link rel="stylesheet" href="styles.css">
    <script src="js/script.js"></script>
    <link rel="icon" href="files/img/icon/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="files/img/icon/favicon.ico" type="image/x-icon">
</head>
<body>
    <div id="mySidenav" class="nav-menu">
        <button class="closebtn" onclick="closeNav()">&times;</button>
        <a href="index.html">Home</a>
        <a href="articles.html">Annotated Articles</a>
        <a href="blog.html">Blog</a>
        <a href="animations.html">Animations</a>
        <!-- <a href="contact.html">Contact Me</a> TODO: Implement Firebase Functionality -->
    </div>
    
    <button class="hamburger" onclick="openNav()" aria-label="Open navigation menu" onKeyDown="handleKeyPress(event)">
        &#9776;
    </button>

    <header>
        <h1>Pay Attention</h1>
        <p>Exploring and annotating the latest research in Generative AI</p>
    </header>

    <main>
        <article>
            <h2>Optimal Brain Damage</h2>
            <p>Published on: <time datetime="1989-01-01">January 1, 1989</time></p>
            <p>Yann Le Cun, John Denker, and Sara Solla examine pruning unimportant weights in this historic paper by computing parameter saliency via second derivative analysis, thereby reducing the number of redundant parameters by a factor of four.</p>
            <a href="articles/Optimal_Brain_Damage.html">Read more</a>
        </article>
        <article>
            <h2>Support-Vector Networks</h2>
            <p>Published on: <time datetime="1995-09-15">September 15, 1995</time></p>
            <p>Corinna Cortes and Vladimir Vapnik revolutionize two group classification with support vectors, derived from the convolution of the dot product (Kernel). The use of this kernel allows for the linear decomposition of a decision surface, which in turn can be optimized using Lagrangians for classification.</p>
            <a href="articles/Support_Vector_Networks.html">Read more</a>
        </article>
        <article>
            <h2>ImageNet Classification with Deep Convolutional Neural Networks</h2>
            <p>Published in: <time datetime="2012">2012</time></p>
            <p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton tackle the issue of multiclass image recognition through developing a revolutionary deep CNN that utilizes GPU threading, ReLU activation, local response normalization, overlapping pooling, patch augmentation, and PCA for RGB channels.</p>
            <a href="articles/ImageNet_Classification.html">Read more</a>
        </article>
        <article>
            <h2>Efficient Estimation of Word Representation in Vector Space</h2>
            <p>Published in: <time datetime="2013">2013</time></p>
            <p>Thomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean examine the lack of meaningful and efficient estimation of word2vector parametrization. Here I explain some of the previous methods (LDA/LSA) and how their approach varies from the previous state-of-the-art.</p>
            <a href="articles/word2vec.html">Read more</a>
        </article>
        <article>
            <h2>Sequence to Sequence Learning with Nerual Networks</h2>
            <p>Published on: <time datetime="2014-12-14">December 14, 2014</time></p>
            <p>Ilya Sutskever, Oriol Vinyals, and Quoc Le tackle utilize an encoder-decoder LSTM network for language translation with four layers. Key insights include reversing the order of source tokens, resulting in state-of-the-art (for the time) BLEU scores.</p>
            <a href="articles/Seq2Seq.html">Read more</a>
        </article>
        <article>
            <h2>Neural Machine Translation by Jointly Learning to Align and Translate</h2>
            <p>Published in: <time datetime="2014-09-01">September 1, 2014</time></p>
            <p>Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio address English-to-French translation using an encoder-decoder GRU-based BiRNN with a novel attention mechanism given bidirectional annotations, outperforming Moses on BLEU translation and planting the seeds for modern attention mechanisms.</p>
           <a href="articles/NMT_via_attention.html">Read more</a>
        </article>
        <article>
            <h2>Neural GPUs Learn Algorithms</h2>
            <p>Published on: <time datetime="2016-11">November 2016</time></p>
            <p>Lukasz Kaiser and Ilya Sutskever tackle the problem of superlinear sequential long addition and long multiplication using a convolutional GRU (CGRU) with grid search, curriculum learning with minibatch, gradient noise, gate cutoff, dropout, and relaxation pull to achieve 100% accuracy.</p>
            <a href="articles/Neural_GPU.html">Read more</a>
        </article>
    </main>

    <footer>
        <p>&copy; 2024 Pay Attention. All rights reserved.</p>
    </footer>
</body>
</html>
