<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pay Attention: Optimal Brain Damage</title>
    <link rel="stylesheet" href="../styles.css"> <!-- Ensure the path is correct -->
    <script src="../js/script.js"></script> <!-- Ensure the path is correct -->
</head>
<body>
    <div id="mySidenav" class="nav-menu">
        <button class="closebtn" onclick="closeNav()">&times;</button>
        <a href="../index.html">Home</a>
        <a href="../articles.html">Articles</a>
        <a href="../blog.html">Blog</a>
        <!-- <a href="../contact.html">Contact Me</a> TODO: Fix firebase -->
    </div>

    <aside>
        <button class="hamburger" onclick="openNav()" aria-label="Open navigation menu" onKeyDown="handleKeyPress(event)">
            &#9776;
        </button>
    </aside>

    <main>
        <article>
            <h2>Optimal Brain Damage</h2>
            <h3>Annotated by: <strong>Noah Schliesman</strong></h3>
            <p>Published on: <time datetime="1989-01-01">January 1, 1989</time> by Yann Le Cun, John Denker, and Sara Solla</p>
            <object data="../files/Optimal_Brain_Damage.pdf" type="application/pdf" width="100%" height="500">
                <p>Please download the annotated PDF to view it: <a href="../files/Optimal_Brain_Damage.pdf">Download PDF</a>.</p>
            </object>
            <h3>Discussion</h3>
            <h4>Abstract</h4>
            <p>The authors begin with claims that removing unimportant weights from a neural net leads to better generalization, fewer training examples required, and improved speed of learning/classification. Intuitively this makes sense, as we are removing redundancies that can contribute to overfitting, hence the clever title <i>Optimal Brain Damage</i>.</p>
            <h3>Introduction</h3>
            <p>They start with a discussion on the success of large models, and how an increase in the number of parameters can lead to overfitting problems. They offer OBD as a way to selectively delete half (or more) weights and end up with a network that can better generalize. After introducing the tradeoff between training error and network complexity, they offer an optimization problem to minimize a cost function composed of these elements. They mention proposed methods of complexity like VC and description length and argue that complexity can be measured as the number of non-zero free parameters.</p>
            <p>The goal is to delete parameters with small salency, which have the least effect on training error.</p>
            <h4>Conclusion</h4>
            <p>This conclusion summarizes the key points of the paper and offers thoughts on future directions for research.</p>
        </article>
    </main>

    <footer>
        <p>&copy; 2024 Pay Attention. All rights reserved.</p>
    </footer>
</body>
</html>
