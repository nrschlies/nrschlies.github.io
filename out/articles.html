<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pay Attention</title>
    <link rel="stylesheet" href="styles.css">
    <script src="js/script.js"></script>
    <link rel="icon" href="files/img/icon/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="files/img/icon/favicon.ico" type="image/x-icon">
</head>
<body>
    <div id="mySidenav" class="nav-menu">
        <button class="closebtn" onclick="closeNav()">&times;</button>
        <a href="index.html">Home</a>
        <a href="articles.html">Annotated Articles</a>
        <a href="blog.html">Blog</a>
        <a href="animations.html">Animations</a>
        <!-- <a href="contact.html">Contact Me</a> TODO: Implement Firebase Functionality -->
    </div>
    
    <button class="hamburger" onclick="openNav()" aria-label="Open navigation menu" onKeyDown="handleKeyPress(event)">
        &#9776;
    </button>

    <header>
        <h1>Pay Attention</h1>
        <p>Exploring and annotating the latest research in Generative AI</p>
    </header>

    <main>
        <article>
            <h2>The Perceptron</h2>
            <p>Published in: <time datetime="1958">1958</time></p>
            <p>Frank Rosenblatt plants the seeds for neural networks with his perceptron, taken as inspiration from Hebb's hypothesis in connectionist neurology. The perceptron is a sum or mean of input stimuli and use statistical seperability to handle both supervised and unsupervised learning via binomial theory.</p>
            <a href="articles/Perceptron.html">Read more</a>
        </article>
        <article>
            <h2>Kalman Filtering</h2>
            <p>Published in: <time datetime="1960">1960</time></p>
            <p>R. E. Kalman tackles the Wiener problem—how does one obtain an original signal given additive, Gaussian noise—through the framework of state spaces in optimal control theory. Such a formulation brings about the basis of modern state space models and my personal research interests entail using Kalman Filtering to improve Neural Networks.</p>
            <a href="articles/Kalman.html">Read more</a>
        </article>
        <article>
            <h2>Learning Representations by Back-propagating Errors</h2>
            <p>Published on: <time datetime="1986-10-09">October 9, 1985</time></p>
            <p>David Rumelhart, Goeffrey Hinton, and Ronald Williams establish the heart of backpropagation by minimizing the difference between the actual output vector and the desired output vector using the accumulation of gradients (as a result of the chain rule).</p>
            <a href="articles/Backpropagation.html">Read more</a>
        </article>
        <article>
            <h2>Estimation of Probabilities from Sparse Data for the Language Model Compoent of a Speech Recognizer</h2>
            <p>Published on: <time datetime="1987-03">March 1987</time></p>
            <p>Slava M. Katz develops a novel nonlinear recursive procedure for redistributing the probability mass of unseen m-grams using Turing discounting. This work was a huge advancement in sparse, memory-efficient language modeling before the prominence of cross-entropy and neural networks.</p>
            <a href="articles/m-grams.html">Read more</a>
        </article>
        <article>
            <h2>A Theoretical Framework for Back-Propagation</h2>
            <p>Published in: <time datetime="1988">1988</time></p>
            <p>Yann Le Cun formalizes backpropagation in Neural Networks using Lagrangian/Hamilton analysis from optimal control theory and generalizes to the cases of weight functions and recurrent networks.</p>
            <a href="articles/Backprop_via_Lagrangians.html">Read more</a>
        </article>
        <article>
            <h2>Optimal Brain Damage</h2>
            <p>Published on: <time datetime="1989-01-01">January 1, 1989</time></p>
            <p>Yann Le Cun, John Denker, and Sara Solla examine pruning unimportant weights in this historic paper by computing parameter saliency via second derivative analysis, thereby reducing the number of redundant parameters by a factor of four.</p>
            <a href="articles/Optimal_Brain_Damage.html">Read more</a>
        </article>
        <article>
            <h2>Learning Long-Term Dependencies with Gradient Descent is Difficult</h2>
            <p>Published in: <time datetime="1994-03">March 1994</time></p>
            <p>Yoshua Bengio, Patrice Simard, and Paolo Frasconi formalize the problem of the vanishing gradient using hyperbolic attractor theory in a recurrent neuron. After extending this to a dynamic system, they offer alternative approaches in simulated annealing, multi-grid random search, time-weighted pseudo-Newton optimization, and discrete error propagation.</p>
            <a href="articles/Vanishing_Gradient.html">Read more</a>
        </article>
        <article>
            <h2>Support-Vector Networks</h2>
            <p>Published on: <time datetime="1995-09-15">September 15, 1995</time></p>
            <p>Corinna Cortes and Vladimir Vapnik revolutionize two group classification with support vectors, derived from the convolution of the dot product (Kernel). The use of this kernel allows for the linear decomposition of a decision surface, which in turn can be optimized using Lagrangians for classification.</p>
            <a href="articles/Support_Vector_Networks.html">Read more</a>
        </article>
        <article>
            <h2>Bidirectional Recurrent Neural Networks</h2>
            <p>Published in: <time datetime="1997-11-">November 1997</time></p>
            <p>Mike Schuster and Kuldip K. Paliwal improve Recurrent Neural Networks (RNNs) by considering the forward states and backward states at the same time during training. These states are independently treated and allow the RNN to better grasp bidirectional context. They use the TIMIT phoneme dataset to achieve state-of-the-art results classifying speech using this new architecture.</p>
            <a href="articles/BRNNs.html">Read more</a>
        </article>
        <article>
            <h2>Greedy Layer-Wise Training of Deep Networks</h2>
            <p>Published in: <time datetime="2006">2006</time></p>
            <p>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle investigate a greedy layer-wise training approach to Deep Belief Networks on a Cotton daataset, Abalone, and MNIST. By using this algorithm for pretraining, weights are better initialized and therefore are less likely to converge to bad extrema.</p>
            <a href"articles/Greedy_Unsupervised_Layerwise_Training.html>Read more</a>
        </article>
        <article>
            <h2>Reducing the Dimensionality of Data with Neural Networks</h2>
            <p>Published on: <time datetime="2006-07-28">July 28, 2006</time></p>
            <p>G. E. Hinton and R. R. Salakhutdinov challenge the traditional methods using PCA to reduce the dimensionality of data. Instead they opt for a Restricted Boltzmann Machine (RBM) based encoder that is greedy layer-wise trained for the encoder. The decoder is the opposite of the encoder and the full autoencoder can be trained using cross-entropy.</p>
            <a href="articles/Autoencoders.html">Read more</a>
        </article>
        <article>
            <h2>Curriculum Learning</h2>
            <p>Published in: <time datetime="2009">2009</time></p>
            <p>Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston enact curriculum learning by teaching a model to learn easier aspects of the task and gradually increasing the difficulty until the model is ready for the test set. This procedure results in better local minima, quicker convergence, and a regulization effect.</p>
            <a href="articles/Curriculum_Learning.html">Read more</a>
        </article>
        <article>
            <h2>ImageNet Classification with Deep Convolutional Neural Networks</h2>
            <p>Published in: <time datetime="2012">2012</time></p>
            <p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton tackle the issue of multiclass image recognition through developing a revolutionary deep CNN that utilizes GPU threading, ReLU activation, local response normalization, overlapping pooling, patch augmentation, and PCA for RGB channels.</p>
            <a href="articles/ImageNet_Classification.html">Read more</a>
        </article>
        <article>
            <h2>Efficient Estimation of Word Representation in Vector Space</h2>
            <p>Published in: <time datetime="2013">2013</time></p>
            <p>Thomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean examine the lack of meaningful and efficient estimation of word2vector parametrization. Here I explain some of the previous methods (LDA/LSA) and how their approach varies from the previous state-of-the-art.</p>
            <a href="articles/word2vec.html">Read more</a>
        </article>
        <article>
            <h2>Sequence to Sequence Learning with Nerual Networks</h2>
            <p>Published on: <time datetime="2014-12-14">December 14, 2014</time></p>
            <p>Ilya Sutskever, Oriol Vinyals, and Quoc Le tackle utilize an encoder-decoder LSTM network for language translation with four layers. Key insights include reversing the order of source tokens, resulting in state-of-the-art (for the time) BLEU scores.</p>
            <a href="articles/Seq2Seq.html">Read more</a>
        </article>
        <article>
            <h2>Neural Machine Translation by Jointly Learning to Align and Translate</h2>
            <p>Published on: <time datetime="2014-09-01">September 1, 2014</time></p>
            <p>Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio address English-to-French translation using an encoder-decoder GRU-based BiRNN with a novel attention mechanism given bidirectional annotations, outperforming Moses on BLEU translation and planting the seeds for modern attention mechanisms.</p>
           <a href="articles/NMT_via_attention.html">Read more</a>
        </article>
        <article>
            <h2>Neural GPUs Learn Algorithms</h2>
            <p>Published in: <time datetime="2016-11">November 2016</time></p>
            <p>Lukasz Kaiser and Ilya Sutskever tackle the problem of superlinear sequential long addition and long multiplication using a convolutional GRU (CGRU) with grid search, curriculum learning with minibatch, gradient noise, gate cutoff, dropout, and relaxation pull to achieve 100% accuracy.</p>
            <a href="articles/Neural_GPU.html">Read more</a>
        </article>
        <article>
            <h2>Attention is All You Need</h2>
            <p>Published in: <time datetime="2017-06">June 2017</time></p>
            <p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaise, and Illia Polosukhin tackle the problem of sequence transduction through a multi-headed dot-product attention mechanism (Transformer) that compares query, key, and values to minimize loss and obtain a state-of-the-art BLEU score of 41.0 using an autoregressive encoder-decoder architecture.</p>
            <a href="articles/Transformer.html">Read more</a>
        </article>
    </main>

    <footer>
        <p>&copy; 2024 Pay Attention. All rights reserved.</p>
    </footer>
</body>
</html>
